---
title: "R Notebook"
output: html_notebook
---

---
title: "Friedman’s Classifier-Based Two-Sample Test (All-in-One)"
author: "Data Scientist"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## 1. Introduction
This R Markdown document demonstrates:
- A simulation study to assess the size and power of Friedman's Classifier-Based Two-Sample Test.
- An application to the HW dataset containing heart-rate zone data with speed (sp.*) and altitude (al.*) measurements.
- Performing Mann-Whitney and Kolmogorov-Smirnov tests on classifier probability scores.

Friedman’s test is performed by:
1. Splitting data from two groups (0 vs 1).
2. Training a classifier to separate them.
3. Obtaining predicted probabilities on a test set.
4. Applying a two-sample test (such as Mann-Whitney or Kolmogorov-Smirnov) to those probability scores.
5. Deciding whether to reject $\(H_0: F_0 = F_1\)$.

## 2. Libraries and Setup

### R Code
```{r}
library(dplyr)
library(tidyr)
library(caret)      # for splitting data, confusionMatrix, etc.
library(glmnet)     # for regularized regression (logistic)
library(e1071)      # confusionMatrix alternative
library(ggplot2)    # Visualization (if needed)
library(stats)      # Nonparametric tests and stats

set.seed(123)  # For reproducibility
```

### Explanation
- `dplyr` and `tidyr` are used for data manipulation.
- `caret` assists in data partitioning and model evaluation.
- `glmnet` is used for logistic regression.
- `e1071` provides alternative confusion matrix functions.
- `ggplot2` is used for visualization if required.
- `stats` provides statistical functions such as the Mann-Whitney and Kolmogorov-Smirnov tests.
- The random seed is set for reproducibility.

## 3. Simulation Study (Size and Power Analysis)

### R Code
```{r}
M <- 500  # Number of simulation replicates
n0 <- 100 # Sample sizes for each class
n1 <- 100
d <- 0.5  # Difference between distributions

pvalues_MW <- numeric(M)
pvalues_KS <- numeric(M)

for(m in 1:M){
  X0 <- rnorm(n0, mean = 0,  sd = 1)  # Class 0
  X1 <- rnorm(n1, mean = d,  sd = 1)  # Class 1
  
  allX <- c(X0, X1)
  allY <- c(rep(0, n0), rep(1, n1))
  
  idx <- sample(seq_along(allY))
  train_idx <- idx[1:round(0.7*(n0+n1))]
  test_idx  <- idx[(round(0.7*(n0+n1))+1):(n0+n1)]
  
  df_train <- data.frame(x=allX[train_idx], y=allY[train_idx])
  df_test  <- data.frame(x=allX[test_idx],  y=allY[test_idx])
  
  # Train logistic regression
  glm_fit <- glm(y ~ x, data=df_train, family=binomial)
  
  # Obtain predicted probabilities on test set
  prob_scores <- predict(glm_fit, newdata=df_test, type="response")
  test_labels <- df_test$y
  
  # Perform two-sample tests on the predicted scores
  scores_0 <- prob_scores[test_labels == 0]
  scores_1 <- prob_scores[test_labels == 1]
  
  pvalues_MW[m] <- wilcox.test(scores_0, scores_1)$p.value
  pvalues_KS[m] <- ks.test(scores_0, scores_1)$p.value
}

# Compute rejection rates
alpha <- 0.05
reject_MW <- mean(pvalues_MW < alpha)
reject_KS <- mean(pvalues_KS < alpha)

cat("Updated Mann-Whitney rejection rate =", reject_MW, "\n")
cat("Updated Kolmogorov-Smirnov rejection rate =", reject_KS, "\n")
```

### Explanation
- Simulates data from two normal distributions: `N(0,1)` and `N(d,1)`.
- Splits data into training (70%) and testing (30%) sets.
- Trains a logistic regression classifier.
- Obtains predicted probabilities for the test set.
- Applies Mann-Whitney and Kolmogorov-Smirnov tests to compare probability distributions.
- Computes empirical rejection rates based on 500 iterations.

### Commentary on Results
- The console prints two updated rejection rates.
- With d=0.5, we expect both tests to reject more often than 0.05, indicating power.
- Example results:
  ```
  Updated Mann-Whitney rejection rate = 0.450
  Updated Kolmogorov-Smirnov rejection rate = 0.370
  ```
  suggest that Mann-Whitney rejects 
  $\(H_0\)$ ~ 45.0% of the time, and KS rejects ~ 37.0% of the time, given d=0.5.
- Setting d=0 would estimate Type I error (should be ~0.05).
- Increasing d should raise rejection rates, showing increased power.

#### 4. Applying Friedman’s Test to the HR Data
Friedman’s approach is applied to a dataset `hw`, where each row represents:
- A zone label `y` (Zone-2, Zone-3, or Zone-4)
- 60 speed measurements (`sp.1` … `sp.60`)
- 60 altitude measurements (`al.1` … `al.60`)

We focus on comparing Zone-2 vs Zone-3.

### 4.1 Load and Inspect the Data
```{r}
# Adjust the file path if needed
# load("hw_data.RData")
# head(hw)
# names(hw)
# dim(hw)
```
**Explanation:**
- Load the dataset `hw_data.RData`.
- Inspect dimensions, column names, and a sample of the data.

### 4.2 Filter to Zone-2 and Zone-3
```{r}
hw_bin <- hw %>%
  filter(y %in% c("Zone-2", "Zone-3")) %>%
  mutate(y_bin = ifelse(y == "Zone-2", 0, 1))

table(hw_bin$y_bin)
```
**Explanation:**
- Filter only rows labeled `Zone-2` or `Zone-3`.
- Convert labels into a binary variable (`0` for Zone-2, `1` for Zone-3).
- Print a table showing the class distribution.

### 4.3.1 Apply the Feature Extraction
```{r}
speed_cols    <- grep("^sp\\.", names(hw_bin), value=TRUE)
altitude_cols <- grep("^al\\.", names(hw_bin), value=TRUE)

list_feats <- lapply(seq_len(nrow(hw_bin)), function(i){
  extract_features(hw_bin[i,], speed_cols, altitude_cols)
})

feature_data <- do.call(rbind, list_feats)
feature_data$y_bin <- hw_bin$y_bin
feature_data <- feature_data %>% drop_na()

dim(feature_data)
head(feature_data)
table(feature_data$y_bin)
```
**Explanation:**
- Identify speed and altitude columns.
- Apply the feature extraction function to each row.
- Combine extracted features into a single dataset.
- Attach the binary class labels and remove any NA rows.
- Display dataset dimensions, a sample of rows, and class distribution.

### 4.4 Friedman’s Steps: Classification and Two-Sample Testing

#### 4.4.1 Split and Train Classifier
```{r}
feature_data$y_bin <- factor(feature_data$y_bin, levels=c(0,1))

set.seed(123)
train_index <- createDataPartition(feature_data$y_bin, p=0.7, list=FALSE)
train_df <- feature_data[train_index, ]
test_df  <- feature_data[-train_index, ]

model_fit <- train(
  y_bin ~ .,
  data = train_df,
  method = "glmnet",
  family = "binomial"
)

prob_scores <- predict(model_fit, newdata=test_df, type="prob")[,2]
true_labels <- test_df$y_bin
```
**Explanation:**
- Convert outcome to a factor.
- Split data into 70% training and 30% testing.
- Train a logistic regression classifier using `glmnet`.
- Obtain predicted probabilities for class 1.

#### 4.4.2 Two-Sample Tests on Probability Scores
```{r}
scores_0 <- prob_scores[true_labels == 0]
scores_1 <- prob_scores[true_labels == 1]

mw_test <- wilcox.test(scores_0, scores_1)
ks_test <- ks.test(scores_0, scores_1)

cat("Mann-Whitney p-value:", mw_test$p.value, "\n")
cat("Kolmogorov-Smirnov p-value:", ks_test$p.value, "\n")
```
**Explanation:**
- Perform Mann-Whitney and Kolmogorov-Smirnov tests on probability scores.
- Print p-values to assess statistical significance.

#### 4.4.3 Confusion Matrix (Optional)
```{r}
pred_classes <- predict(model_fit, newdata=test_df)
conf_mat <- confusionMatrix(pred_classes, test_df$y_bin)
conf_mat
```

