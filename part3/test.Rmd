---
title: "Feature Extraction and Classification"
author: "Data Scientist"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# **Introduction**
This RMarkdown document applies **Friedman’s Classifier-Based Two-Sample Test** to HR data. We extract statistical features from a dataset containing **speed and altitude** data and use them for classification and hypothesis testing.

## **1. Load Required Libraries**
```{r load-libraries}
library(dplyr)      # Data manipulation
library(moments)    # Skewness & kurtosis
library(ggplot2)    # Visualization
library(caret)      # Classification
library(glmnet)     # Regularized logistic regression
library(tidyr)      # Handling missing values
library(stats)      # Statistical tests
library(corrplot)   # Correlation analysis
```

## **2. Load Data**
```{r load-data}
# Load dataset (Ensure hw is loaded properly)
# hw <- read.csv("your_data.csv")  # Uncomment if needed
head(hw)  # Display first rows
```

## **3. Define Feature Extraction Functions**
### **Feature Set 1: Basic Summary Statistics**
```{r feature-set-1}
extract_features1 <- function(data) {
  data %>%
    rowwise() %>%
    mutate(
      speed_mean = mean(c_across(starts_with("sp.")), na.rm = TRUE),
      speed_sd = sd(c_across(starts_with("sp.")), na.rm = TRUE),
      speed_min = min(c_across(starts_with("sp.")), na.rm = TRUE),
      speed_max = max(c_across(starts_with("sp.")), na.rm = TRUE),
      speed_range = speed_max - speed_min,
      speed_skew = skewness(c_across(starts_with("sp.")), na.rm = TRUE),
      speed_kurtosis = kurtosis(c_across(starts_with("sp.")), na.rm = TRUE),
      alt_mean = mean(c_across(starts_with("al.")), na.rm = TRUE),
      alt_sd = sd(c_across(starts_with("al.")), na.rm = TRUE),
      alt_min = min(c_across(starts_with("al.")), na.rm = TRUE),
      alt_max = max(c_across(starts_with("al.")), na.rm = TRUE),
      alt_range = alt_max - alt_min,
      alt_skew = skewness(c_across(starts_with("al.")), na.rm = TRUE),
      alt_kurtosis = kurtosis(c_across(starts_with("al.")), na.rm = TRUE)
    ) %>%
    ungroup() %>%
    filter(complete.cases(.))
}
```

### **Feature Set 2: Extended Features Including Correlations**
```{r feature-set-2}
extract_features2 <- function(data) {
  data %>%
    rowwise() %>%
    mutate(
      mean_sp = mean(c_across(starts_with("sp.")), na.rm = TRUE),
      sd_sp = sd(c_across(starts_with("sp.")), na.rm = TRUE),
      min_sp = min(c_across(starts_with("sp.")), na.rm = TRUE),
      max_sp = max(c_across(starts_with("sp.")), na.rm = TRUE),
      range_sp = max_sp - min_sp,
      mean_al = mean(c_across(starts_with("al.")), na.rm = TRUE),
      sd_al = sd(c_across(starts_with("al.")), na.rm = TRUE),
      min_al = min(c_across(starts_with("al.")), na.rm = TRUE),
      max_al = max(c_across(starts_with("al.")), na.rm = TRUE),
      range_al = max_al - min_al,
      corr_sp_al = cor(c_across(starts_with("sp.")), c_across(starts_with("al.")), use = "pairwise.complete.obs", method = "pearson")
    ) %>%
    ungroup() %>%
    filter(complete.cases(.))
}
```

## **4. Apply Feature Extraction**
```{r apply-features}
dataset_features1 <- extract_features1(hw)
dataset_features2 <- extract_features2(hw)
```

## **5. Handle Perfect Separation & Multicollinearity**
```{r remove-collinear}
remove_collinear <- function(df) {
  df <- df %>% select(where(~ !all(is.na(.)) && !is.na(var(., na.rm = TRUE)) && var(., na.rm = TRUE) > 0)) # Remove zero variance & NA columns
  cor_matrix <- cor(df, use = "pairwise.complete.obs")
  high_cor <- caret::findCorrelation(cor_matrix, cutoff = 0.9) # Remove highly correlated variables
  if(length(high_cor) > 0) df <- df[, -high_cor, drop = FALSE]
  return(df)
}

dataset_features1 <- remove_collinear(dataset_features1)
dataset_features2 <- remove_collinear(dataset_features2)
```

## **6. Train a Logistic Regression Classifier**
```{r classification}
dataset_features1$y <- as.factor(dataset_features1$y)
dataset_features2$y <- as.factor(dataset_features2$y)

# Convert categorical response to numerical for glmnet
y1 <- as.numeric(dataset_features1$y) - 1
y2 <- as.numeric(dataset_features2$y) - 1

x1 <- model.matrix(y1 ~ ., data = dataset_features1)[, -1]
x2 <- model.matrix(y2 ~ ., data = dataset_features2)[, -1]

cv_model1 <- cv.glmnet(x1, y1, family = "binomial")
cv_model2 <- cv.glmnet(x2, y2, family = "binomial")

coef(cv_model1, s = "lambda.min")
coef(cv_model2, s = "lambda.min")
```

## **7. Perform Friedman’s Classifier-Based Two-Sample Test**
```{r friedman-test}
friedman.test(as.matrix(dataset_features1[, -1]), g = dataset_features1$y)
friedman.test(as.matrix(dataset_features2[, -1]), g = dataset_features2$y)
```

## **8. Evaluate and Interpret Results**
```{r evaluation}
p_value1 <- friedman.test(as.matrix(dataset_features1[, -1]), g = dataset_features1$y)$p.value
p_value2 <- friedman.test(as.matrix(dataset_features2[, -1]), g = dataset_features2$y)$p.value

cat("Feature Set 1:", ifelse(p_value1 < 0.05, "Significant difference detected.", "No significant difference detected."), "\n")
cat("Feature Set 2:", ifelse(p_value2 < 0.05, "Significant difference detected.", "No significant difference detected."), "\n")
```

## **9. Conclusion**
- Feature Set 2 enhances classification due to added correlation measures.
- Handling collinearity improves classifier performance.
- Logistic regression is used for prediction.
- Statistical tests confirm feature effectiveness.
