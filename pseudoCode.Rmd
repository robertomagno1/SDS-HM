---
title: "SDS HM part 2"
output: html_notebook
---

#Data Preparation

1. Combine both samples into a single dataset:
   - \( U = X \cup Z \)
2. Assign labels:
   - \( Y = 1 \) for \( X \) (first sample)
   - \( Y = -1 \) for \( Z \) (second sample)
3. Preprocess the data (optional):
   - Normalize or standardize features if necessary
   - Check for missing values and handle them
   
   
# Training the Binary Classifier

4. Split dataset \( U, Y \) into:
   - Training set \( (U_{\text{train}}, Y_{\text{train}}) \)
   - Testing set \( (U_{\text{test}}, Y_{\text{test}}) \)
5. Train a binary classifier \( F(U) \) to distinguish between \( X \) and \( Z \)
   - Options: Logistic Regression, SVM, Random Forest, Neural Network
   - Optimize hyperparameters to avoid overfitting
6. Apply the trained classifier to the test set:
   - Compute prediction scores \( S = F(U_{\text{test}}) \)

   
# Extract score distribution:

7. Extract prediction scores for each group:
   - \( S_X = \{ S_i \mid Y_i = 1 \} \)  (Scores from \( X \))
   - \( S_Z = \{ S_i \mid Y_i = -1 \} \) (Scores from \( Z \))
8. Verify if \( S_X \) and \( S_Z \) follow different distributions.


# Apply Statistical Tests on Score Distributions

9. Perform **Mann-Whitney U Test** on \( S_X, S_Z \):
   - Tests if the two distributions have the same median.
   - Returns \( p_{\text{MW}} \).

10. Perform **Kolmogorov-Smirnov (KS) Test** on \( S_X, S_Z \):
   - Tests if the two distributions are identical.
   - Returns \( p_{\text{KS}} \).

11. Perform **Permutation Test**:
   - Shuffle labels \( Y \) multiple times (P times)
   - Train classifier and recompute test statistic each time
   - Compute empirical \( p \)-value \( p_{\text{Perm}} \)

# Decision Making
12. Compute final decision:
    - If \( p_{\text{MW}} < \alpha \) OR \( p_{\text{KS}} < \alpha \) OR \( p_{\text{Perm}} < \alpha \):
        - Reject \( H_0 \) (distributions are different).
    - Else:
        - Do not reject \( H_0 \) (no significant difference detected).


# Why Mann-Whitney and Kolmogorov-Smirnov as Baseline Tests?
Friedman suggests using Mann-Whitney U and Kolmogorov-Smirnov tests because:

They are non-parametric:

Do not assume a specific distribution of the data.
Work well in high-dimensional settings.
They are robust:

Can detect differences in location (median) and shape of distributions.
Effective even with small sample sizes.
They complement each other:

Mann-Whitney U is powerful when distributions differ in central tendency.
Kolmogorov-Smirnov is effective when distributions differ in cumulative shape.
They allow easy computation of the null distribution:

Can be used directly if separate training and testing sets are maintained.
If the same data is used for both training and scoring, permutation testing corrects for overfitting.



# We choose to start with Logistic Regression (glm) since:

1 - It is easy to interpret.
2 - Works well in low-dimensional settings.
3 - Can be regularized (L1/L2) to avoid overfitting.





```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

