---
title: "Basic Power Analysis with Logistic Regression"
author: "Data Science Implementation"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

A two-sample test is a statistical test used to check whether two independent datasets come from the same probability distribution.

Let’s define our datasets:
$$
\text{Let } X = \{X_1, X_2, ..., X_n\} \sim F_X, \quad Y = \{Y_1, Y_2, ..., Y_m\} \sim F_Y
$$
We test the hypotheses:
$$
H_0: F_X = F_Y \quad \text{(The two distributions are the same)}
$$

$$
H_1: F_X \neq F_Y \quad \text{(The two distributions are different)}
$$
##### Dataset choice

We need a dataset that:
- Simulates real-world variability: The data should contain structured differences between two populations while maintaining some overlap.
- Allows for easy visualization and interpretation: Since we're exploring two-sample testing, we want a dataset with two features that can be plotted in 2D.
- Introduces controlled differences between the two distributions: By setting different mean vectors and covariance structures, we ensure that the two groups are similar but not identical. Our goal is to see how well a classifier can distinguish between the two groups.
```{r}
set.seed(22) # my lucky number
n_samples <- 500  # Number of Data points per group, not too many, not too little
n_features <- 2   # Number of dimensions(aka features, variables)
```

A multivariate normal distribution is a generalization of the normal distribution to multiple dimensions. It is defined as:
$$
X \sim N(\mu_X, \Sigma_X), \quad Y \sim N(\mu_Y, \Sigma_Y)
$$
where:
- \mu's are the mean vectors (determining the center of the distributions).
- \sim's are the covariance matrices (determining the shape and spread of the distributions).

The covariance matrix influences the shape of the data cloud:
$$
\Sigma =
\begin{bmatrix}
\sigma_1^2 & \rho \sigma_1 \sigma_2 \\
\rho \sigma_1 \sigma_2 & \sigma_2^2
\end{bmatrix}
$$
where:
- \sigma_1^2 & \sigma_2^2 &are variances of the two features.
- \rho is the correlation coefficient.

```{r}
# means vectors for X and Y (where both distribution are centered) on cartesian map
mu_X <- c(2, 2)
mu_Y <- c(0, 0)
# covariance matrices
Sigma_X <- matrix(c(1, 0.5, 0.5, 1), nrow=2)  # Correlated variables for X
Sigma_Y <- matrix(c(1, -0.3, -0.3, 1), nrow=2)  # Slightly negatively correlated for Y
# ensure different internal structures
```

```{r}
library("MASS") 
#?mvrnorm
# multivariate normal samples
X <- mvrnorm(n_samples, mu_X, Sigma_X)
Y <- mvrnorm(n_samples, mu_Y, Sigma_Y)

df_X <- as.data.frame(X)
df_Y <- as.data.frame(Y)
df_X$group <- "X"
df_Y$group <- "Y"
df <- rbind(df_X, df_Y)
colnames(df) <- c("Feature1", "Feature2", "Group")
df$Group <- as.factor(df$Group)
print(df)
```
```{r}
summary(df)
```

```{r}
library(ggplot2)
ggplot(df, aes(x = Feature1, y = Feature2, color = Group)) +
  geom_point(alpha = 0.6, size = 2) +
  labs(title = "Scatter Plot of X and Y", x = "Feature 1", y = "Feature 2") +
  theme_minimal()

```
Th
```{r}
#install.packages("ggpubr")
library(ggpubr)

p1 <- ggplot(df, aes(x = Feature1, fill = Group)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 30) +
  labs(title = "Distribution of Feature 1", x = "Feature 1", y = "Count") +
  theme_minimal()

p2 <- ggplot(df, aes(x = Feature2, fill = Group)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 30) +
  labs(title = "Distribution of Feature 2", x = "Feature 2", y = "Count") +
  theme_minimal()

ggarrange(p1, p2, ncol = 2, nrow = 1)
```

```{r}
ggplot(df, aes(x = Group, y = Feature1, fill = Group)) +
  geom_boxplot(alpha = 0.6) +
  labs(title = "Boxplot of Feature 1 by Group", y = "Feature 1") +
  theme_minimal()

```

```{r}
ggplot(df, aes(x = Feature1, y = Feature2, color = Group)) +
  stat_ellipse(level = 0.95, geom = "polygon", alpha = 0.2) +
  geom_point(alpha = 0.6, size = 2) +
  labs(title = "Covariance Ellipses for X and Y", x = "Feature 1", y = "Feature 2") +
  theme_minimal()

```
Now we can start the main part
```{r}
# Shuffle data to prevent order bias
set.seed(2) # my second lucky number
df <- df[sample(nrow(df)), ]
print(df)
```
We train a Binary Classifier that learns to distinguish between X and Y.

So if Fx and Fy:
- are different, the classifier will separate them
- are similar, the classifier will perform poorly

```{r}
library(caret)
library(rpart)
# This time we avoid tuning the hyperparameters since it's only a PseudoCode or even better a demonstration on a toy dataset
# Split 80/20
train_index <- createDataPartition(df$Group, p = 0.8, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]

# CV
cv_control <- trainControl(method = "cv", number = 5)

# Train 
tree_classifier <- train(Group ~ Feature1 + Feature2, 
                         data = train_data, 
                         method = "rpart", 
                         trControl = cv_control,
                         tuneGrid = expand.grid(cp = 0.02)  # Increase complexity parameter, this serve as a sort of regularization to avoid overfitting, in easier term we prune the tree by restricting its depth
)
```

```{r}
library(rpart.plot)
rpart.plot(tree_classifier$finalModel, 
           main = "Decision Tree Splits (Cross-Validated)")

```
For each sample, the classifier assigns a probability score
$$
s_i = P(\text{sample } i \text{ belongs to class } 1)
$$

```{r}
library(caret)
scores <- predict(tree_classifier, df, type = "prob")[,2]  # Probabilities for class 1
```

Compute scores separately for X and Y:
$$
S_X = \{s_1, s_2, \dots, s_n\}, \quad S_Y = \{s_1, s_2, \dots, s_m\}
$$

```{r}
scores <- predict(classifier, df, type = "prob")[,2]  # Probabilities for class 1
scores_X <- test_probs[test_data$Group == "X"]
scores_Y <- test_probs[test_data$Group == "Y"]

score_df <- data.frame(
  Scores = c(scores_X, scores_Y),
  Label = rep(c("X", "Y"), c(length(scores_X), length(scores_Y)))
)

# Plot 
ggplot(score_df, aes(x = Scores, fill = Label)) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 30) +
  labs(title = "Classification Scores (Test Set)", x = "Score", y = "Count") +
  theme_minimal()
```
```{r}
library(pROC)
test_probs <- predict(tree_classifier, test_data, type = "prob")[,2]
test_labels <- as.numeric(test_data$Group) - 1 
roc_obj <- roc(test_labels, test_probs)
roc_obj
#Plot
plot(roc_obj, col = "blue", lwd = 2, main = "ROC Curve")
```
Since we now have a single score per sample, apply a Univariate Two-Sample Test.
The paper mention two methods:

##### Mann-Whitney U Test
$$
U = n_X n_Y + \frac{n_X (n_X + 1)}{2} - R_X
$$
where:
- R_X: sum of ranks assigned to X values

It compares medians using ranked data and is suitable when distributions are similar but shifted.
```{r}
wilcox_test <- wilcox.test(scores_X, scores_Y, exact = FALSE)
wilcox_test
```
Edit: Printing the whole function and not the actual value allowed us to discover the real p value, I think R by default cut the value after a certain approximation so when I saw that both the values were 0 it was a little bit misleading, instead discovering the real value is reaaaally small but still a legit value was important.

##### Kolmogorov-Smirnov (KS) Test
$$
D = \sup_x | F_X(x) - F_Y(x) |
$$
where:
- F_X(x) and F_Y(x) are empirical CDFs

Measures the maximum difference between cumulative distribution functions (CDFs).
Detects broader distributional differences
```{r}
ks_test <- ks.test(scores_X, scores_Y, exact = FALSE) 
ks_test
```
Avvertimento: il p-value sarà approssimativo in presenza di legami[1]
We had this warning (sorry if it's italian), anyway exact = FALSE should deal with this issue (the test detected ties (identical values) in the data), it's caused by the simplicity of the dataset we choosed to analyze and consequentially the decision tree split assign identical scores to multiple data points (The decision tree provides discrete probability outputs)

```{r}
alpha <- 0.05  # Significance level

# Step 5: Decision Rule
if (wilcox_test$p.value < alpha) {
    print("Mann-Whitney U Test: Reject H0, distributions are significantly different")
} else {
    print("Mann-Whitney U Test: Fail to reject H0, no significant difference")
}

if (ks_test$p.value < alpha) {
    print("Kolmogorov-Smirnov Test: Reject H0, distributions are significantly different")
} else {
    print("Kolmogorov-Smirnov Test: Fail to reject H0, no significant difference")
}
```
Anyway both results indicate strong evidence against the null hypothesis (H0:X=Y), meaning that the two distributions are significantly different. 
Specifically Kolmogorov-Smirnov Test p-value: close to 0 indicates extreme separation between distributions The classifier assigns very different probability scores to X and Y meaning their empirical CDFs do not overlap at all
In practical terms, the test detects a almost perfect separation.

Let's visualize it
```{r}
ecdf_X <- ecdf(scores_X)
ecdf_Y <- ecdf(scores_Y)

# Plot 
plot(ecdf_X, col = "blue", lwd = 2, main = "Empirical CDFs of Classification Scores", xlab = "Score", ylab = "Cumulative Probability")
lines(ecdf_Y, col = "red", lwd = 2)
legend("bottomright", legend = c("X", "Y"), col = c("blue", "red"), lwd = 2)

```
Nonetheless the regularizion the decision tree is acting like a hard rule-based classifier, meaning it lacks generalization

Let's try to use a Less Deterministic Classifier like Logistic Regression to get softer probability estimates
```{r}
logistic_model <- train(Group ~ Feature1 + Feature2, data = train_data, method = "glm", family = "binomial")
test_probs_logistic <- predict(logistic_model, test_data, type = "prob")[,2]

# Plot
ecdf_X_logistic <- ecdf(test_probs_logistic[test_data$Group == "X"])
ecdf_Y_logistic <- ecdf(test_probs_logistic[test_data$Group == "Y"])

plot(ecdf_X_logistic, col = "blue", lwd = 2, main = "Empirical CDFs (Logistic Regression)", xlab = "Score", ylab = "Cumulative Probability")
lines(ecdf_Y_logistic, col = "red", lwd = 2)
legend("bottomright", legend = c("X", "Y"), col = c("blue", "red"), lwd = 2)

```
Logistic Regression provides a much more realistic probability distribution. We can see some natural overlap between X and Y making the tests valid since the classifier is no longer overconfident
```{r}
# Mann-Whitney U Test
wilcox_test <- wilcox.test(test_probs_logistic[test_data$Group == "X"], 
                           test_probs_logistic[test_data$Group == "Y"], 
                           exact = FALSE)
wilcox_test
```
```{r}
# Kolmogorov-Smirnov Test
ks_test <- ks.test(test_probs_logistic[test_data$Group == "X"], 
                   test_probs_logistic[test_data$Group == "Y"])
ks_test
```

```{r}
# Decision Rule
if (wilcox_test$p.value < alpha) {
    print("Mann-Whitney U Test: Reject H0, distributions are significantly different")
} else {
    print("Mann-Whitney U Test: Fail to reject H0, no significant difference")
}

if (ks_test$p.value < alpha) {
    print("Kolmogorov-Smirnov Test: Reject H0, distributions are significantly different")
} else {
    print("Kolmogorov-Smirnov Test: Fail to reject H0, no significant difference")
}
```

Even with logistic regression, the classifier separates X and Y extremely well.
This suggests that our dataset is highly distinguishable, making most two-sample tests extremely sensitive.

```{r}
roc_obj <- roc(as.numeric(test_data$Group) - 1, test_probs_logistic)
roc_obj
plot(roc_obj, col = "blue", lwd = 2, main = "ROC Curve (Logistic Regression)")
abline(a = 0, b = 1, lty = 2, col = "gray")

```
AUC is close to 1 but not exactly 1, meaning the classifier is working well.





