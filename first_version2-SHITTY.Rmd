---
title: "Basic Power Analysis with Logistic Regression"
author: "Data Science Implementation"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## **Introduction**
This document implements a basic power analysis simulation using **logistic regression** without optimizations.

---

## **1️⃣ Load Libraries & Data**
```{r}
library(tidyverse)
library(caret)
library(MASS)
```
```{r load-data}
# Load dataset (Assuming `hw` dataset is already available)
str(hw)  # Check structure
```

### **Filter Data: Remove Missing Classes**
```{r}
hw <- hw %>% filter(y %in% c("Zone-2", "Zone-3"))
table(hw$y)  # Ensure only two classes remain
```

---
## **2️⃣ Feature Selection & Preprocessing**
```{r}
# Convert dataset to tibble to avoid select() issues
hw <- as_tibble(hw)


# Separate features and target using Base R to avoid select() issues
X <- hw[, !(names(hw) %in% "y")]
y <- hw$y
```
---
## **3️⃣ Train-Test Split (70/30)**
```{r}
set.seed(123)
trainIndex <- createDataPartition(y, p = 0.7, list = FALSE)
trainData <- X[trainIndex, ]
testData <- X[-trainIndex, ]
trainLabels <- y[trainIndex]
testLabels <- y[-trainIndex]
```

---
## **4️⃣ Train Basic Logistic Regression**
```{r}
model <- glm(trainLabels ~ ., data = as.data.frame(trainData), family = binomial)
```

---
## **5️⃣ Compute Scores & Apply Statistical Tests**
```{r}
# Predict on test data
scores <- predict(model, as.data.frame(testData), type = "response")
scores_zone2 <- scores[testLabels == "Zone-2"]
scores_zone3 <- scores[testLabels == "Zone-3"]

# Mann-Whitney U Test
mw_test <- wilcox.test(scores_zone2, scores_zone3)
mw_test$p.value

# Kolmogorov-Smirnov Test
ks_test <- ks.test(scores_zone2, scores_zone3)
ks_test$p.value
```

---
## **6️⃣ Power Simulation Without Optimization**
```{r}
simulate_power <- function(mu_shift, n0, n1, k, num_sim = 500) {
  power <- 0
  for (i in 1:num_sim) {
    X0 <- mvrnorm(n0, mu = rep(0, k), Sigma = diag(k))
    X1 <- mvrnorm(n1, mu = rep(mu_shift, k), Sigma = diag(k))
    data <- data.frame(rbind(X0, X1), class = as.factor(c(rep(0, n0), rep(1, n1))))
    model <- glm(class ~ ., data = data, family = binomial)
    scores <- predict(model, data, type = "response")
    p_value <- wilcox.test(scores[1:n0], scores[(n0 + 1):(n0 + n1)])$p.value
    if (p_value < 0.05) {
      power <- power + 1
    }
  }
  return(power / num_sim)
}
```

---
## **7️⃣ Compute Power Curve & Plot**
```{r}
power_values <- sapply(seq(0, 2, by = 0.2), function(d) simulate_power(d, 50, 50, 4, 100))

plot(seq(0, 2, by = 0.2), power_values, type = "b", 
     main = "Power vs. Mean Shift",
     xlab = "Mean Shift (Δ)", ylab = "Power", 
     col = "blue", pch = 19)
```


### **Filter Data: Remove Missing Classes**
```{r}
hw <- hw %>% filter(y %in% c("Zone-2", "Zone-3"))
table(hw$y)  # Ensure only two classes remain
```

---
## **2️⃣ Feature Selection & Preprocessing**
```{r}
# Ensure tibble format
hw <- as_tibble(hw)

# Separate features and target using Base R to avoid select() issues
X <- hw[, !(names(hw) %in% "y")]
y <- hw$y
```

---
## **3️⃣ Train-Test Split (70/30)**
```{r}
set.seed(123)
trainIndex <- createDataPartition(y, p = 0.7, list = FALSE)
trainData <- X[trainIndex, ]
testData <- X[-trainIndex, ]
trainLabels <- y[trainIndex]
testLabels <- y[-trainIndex]
```

---
## **4️⃣ Train Basic Logistic Regression**
```{r}
model <- glm(trainLabels ~ ., data = as.data.frame(trainData), family = binomial)
```

---
## **5️⃣ Compute Scores & Apply Statistical Tests**
```{r}
# Predict on test data
scores <- predict(model, as.data.frame(testData), type = "response")
scores_zone2 <- scores[testLabels == "Zone-2"]
scores_zone3 <- scores[testLabels == "Zone-3"]

# Mann-Whitney U Test
mw_test <- wilcox.test(scores_zone2, scores_zone3)
mw_test$p.value

# Kolmogorov-Smirnov Test
ks_test <- ks.test(scores_zone2, scores_zone3)
ks_test$p.value
```

---
## **6️⃣ Power Simulation Without Optimization**
```{r}
simulate_power <- function(mu_shift, n0, n1, k, num_sim = 500) {
  power <- 0
  for (i in 1:num_sim) {
    X0 <- mvrnorm(n0, mu = rep(0, k), Sigma = diag(k))
    X1 <- mvrnorm(n1, mu = rep(mu_shift, k), Sigma = diag(k))
    data <- data.frame(rbind(X0, X1), class = as.factor(c(rep(0, n0), rep(1, n1))))
    model <- glm(class ~ ., data = data, family = binomial)
    scores <- predict(model, data, type = "response")
    p_value <- wilcox.test(scores[1:n0], scores[(n0 + 1):(n0 + n1)])$p.value
    if (p_value < 0.05) {
      power <- power + 1
    }
  }
  return(power / num_sim)
}
```

---
## **7️⃣ Compute Power Curve & Plot**
```{r}
power_values <- sapply(seq(0, 2, by = 0.2), function(d) simulate_power(d, 50, 50, 4, 100))

plot(seq(0, 2, by = 0.2), power_values, type = "b", 
     main = "Power vs. Mean Shift",
     xlab = "Mean Shift (Δ)", ylab = "Power", 
     col = "blue", pch = 19)
```
---
title: "Optimized 
author: "Data Science Implementation"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(MASS)
library(glmnet)     # LASSO regression
library(doParallel) # Parallel processing
library(ROCR)       # ROC curve analysis

# Load dataset (Assume `hw` is preloaded)
str(hw)  

# Ensure only "Zone-2" and "Zone-3" are included
hw <- hw %>% filter(y %in% c("Zone-2", "Zone-3"))
hw$y <- factor(hw$y)  # Convert to factor
table(hw$y)           # Confirm balance
```


```{r}
# Extract feature matrix and target variable
X <- hw[, !(names(hw) %in% "y")]
y <- hw$y

```

```{r}
set.seed(123)
trainIndex <- createDataPartition(y, p = 0.7, list = FALSE)

if (!is.null(trainIndex)) {
  trainData <- X[trainIndex, ]
  testData <- X[-trainIndex, ]
  trainLabels <- y[trainIndex]
  testLabels <- y[-trainIndex]
} else {
  stop("Error: No valid data for partitioning. Check class balance.")
}

# Prepare data for glmnet (LASSO logistic regression)
trainMatrix <- as.matrix(trainData)
testMatrix <- as.matrix(testData)

# Train LASSO model
cv_fit <- cv.glmnet(trainMatrix, trainLabels, family = "binomial", alpha = 1)
model <- glmnet(trainMatrix, trainLabels, family = "binomial", alpha = 1, lambda = cv_fit$lambda.min)

```

# Predict on test set
```{r}
scores <- predict(model, testMatrix, type = "response")
scores_zone2 <- scores[testLabels == "Zone-2"]
scores_zone3 <- scores[testLabels == "Zone-3"]

# Mann-Whitney U Test
mw_test <- wilcox.test(scores_zone2, scores_zone3)
mw_test$p.value

# Kolmogorov-Smirnov Test
ks_test <- ks.test(scores_zone2, scores_zone3)
ks_test$p.value

```
# Register parallel backend
```{r}
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

simulate_power_optimized <- function(mu_shift, n0, n1, k, num_sim = 500) {
  power <- foreach(i = 1:num_sim, .combine = '+') %dopar% {
    X0 <- mvrnorm(n0, mu = rep(0, k), Sigma = diag(k))
    X1 <- mvrnorm(n1, mu = rep(mu_shift, k), Sigma = diag(k))
    data <- data.frame(rbind(X0, X1), class = as.factor(c(rep(0, n0), rep(1, n1))))
    model <- glm(class ~ ., data = data, family = binomial)
    scores <- predict(model, data, type = "response")
    p_value <- wilcox.test(scores[1:n0], scores[(n0 + 1):(n0 + n1)])$p.value
    as.integer(p_value < 0.05)
  }
  return(power / num_sim)
}

stopCluster(cl)  # Stop parallel cluster
```


```{r}

library(doParallel)
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)


power_values <- sapply(seq(0, 2, by = 0.2), function(d) simulate_power_optimized(d, 50, 50, 4, 100))

plot(seq(0, 2, by = 0.2), power_values, type = "b", 
     main = "Optimized Power vs. Mean Shift",
     xlab = "Mean Shift (Δ)", ylab = "Power", 
     col = "red", pch = 19)
```


---
title: "Optimized Logistic Regression & Power Analysis"
output: html_document


```{r setup, include=FALSE}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(caret)
library(MASS)
library(doParallel)
knitr::opts_chunk$set(echo = TRUE)
```

## **Data Preprocessing**

```{r load-data}
# Load dataset (Assume hw is preloaded)
str(hw)
```

```{r data-cleaning}
# Filter to include only 'Zone-2' and 'Zone-3' and convert 'y' to a factor
hw <- hw %>% filter(y %in% c("Zone-2", "Zone-3"))
hw$y <- factor(hw$y)

# Display table to check balance
table(hw$y)
```

```{r data-scaling}
# Scale the dataset excluding the target variable 'y'
hw_scaled <- as.data.frame(scale(hw %>% dplyr::select(-y)))
# Add back the 'y' column
hw_scaled$y <- hw$y
```

## **Train-Test Split**

```{r split-data}
set.seed(123)
trainIndex <- createDataPartition(hw_scaled$y, p = 0.7, list = FALSE)
trainData <- hw_scaled[trainIndex, ]
testData <- hw_scaled[-trainIndex, ]
```

## **Train Logistic Regression Model**

```{r train-model}
# Train Logistic Regression model
model <- glm(y ~ ., data = trainData, family = binomial, control = list(maxit = 1000))
summary(model)
```

## **Predictions and Performance Evaluation**

```{r predict-evaluate}
# Predict probabilities on the test dataset
scores <- predict(model, testData, type = "response")

# Extract prediction scores per HR zone
scores_zone2 <- scores[testData$y == "Zone-2"]
scores_zone3 <- scores[testData$y == "Zone-3"]

# Perform Mann-Whitney U Test
mw_test <- wilcox.test(scores_zone2, scores_zone3)
mw_test$p.value

# Perform Kolmogorov-Smirnov Test
ks_test <- ks.test(scores_zone2, scores_zone3)
ks_test$p.value
```

## **Permutation Test**

```{r permutation-test}
perm_test <- function(scores_0, scores_1, num_perm = 1000) {
  observed_stat <- abs(mean(scores_0) - mean(scores_1))
  perm_stats <- replicate(num_perm, {
    combined <- sample(c(scores_0, scores_1))
    perm_stat <- abs(mean(combined[1:length(scores_0)]) - mean(combined[(length(scores_0) + 1):length(combined)]))
    return(perm_stat)
  })
  p_value <- mean(perm_stats >= observed_stat)
  return(p_value)
}

# Run permutation test
perm_test(scores_zone2, scores_zone3)
```

## **Power Simulation Study**

```{r power-simulation}
simulate_power_optimized <- function(mu_shift, n0, n1, k, num_sim = 500) {
  power <- 0
  for (i in 1:num_sim) {
    X0 <- mvrnorm(n0, mu = rep(0, k), Sigma = diag(k))
    X1 <- mvrnorm(n1, mu = rep(mu_shift, k), Sigma = diag(k))
    data <- data.frame(rbind(X0, X1), class = as.factor(c(rep(0, n0), rep(1, n1))))
    model <- glm(class ~ ., data = data, family = binomial, control = list(maxit = 1000))
    scores <- predict(model, data, type = "response")
    p_value <- wilcox.test(scores[1:n0], scores[(n0 + 1):(n0 + n1)])$p.value
    power <- power + as.integer(p_value < 0.05)
  }
  return(power / num_sim)
}
```

```{r plot-power}
# Run power analysis for different mean shifts
power_values <- sapply(seq(0, 2, by = 0.2), function(d) simulate_power_optimized(d, 50, 50, 4, 100))

# Plot Power Curve
plot(seq(0, 2, by = 0.2), power_values, type = "b",
     main = "Power vs. Mean Shift", xlab = "Mean Shift (Δ)", ylab = "Power",
     col = "blue", pch = 19)
```
